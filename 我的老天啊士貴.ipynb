{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 1.<b>前置作業</b> </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 注意：要先載ckiptagger </h4>\n",
    "<ul>\n",
    "  <li> <a href=\"https://github.com/ckiplab/ckiptagger\">Github官方文檔</a></li>\n",
    "  <li> <a href=\"https://medium.com/@br19920702/%E4%B8%AD%E7%A0%94%E9%99%A2%E9%96%8B%E6%BA%90nlp%E5%A5%97%E4%BB%B6-ckiptagger-%E7%B9%81%E4%B8%AD%E4%B8%8D%E7%B5%90%E5%B7%B4-7822fc4efbf\">中文安裝教學</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm(list=ls())\n",
    "for v in dir(): del globals()[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('C:\\\\Users\\\\chiaming\\\\Desktop\\\\python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiaming\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\chiaming\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\chiaming\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\chiaming\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\chiaming\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\chiaming\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from ckiptagger import WS, POS, NER\n",
    "ws = WS(\"./data\")\n",
    "pos = POS(\"./data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_small = pd.read_csv('C:\\\\Users\\\\chiaming\\\\Desktop\\\\python\\\\士貴\\\\專案資料.csv',engine='python',encoding='utf-8',header=None,names=['日期','標題','內容'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#ffffff\"> <b>>預先設置函數 </b></p>\n",
    "<ul>\n",
    "  <li style=\"color:#ffffff\">辭典權重</li>\n",
    "\n",
    "  <li style=\"color:#ffffff\">停止詞、停止詞詞性</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dictionary(word_to_weight):\n",
    "    length_word_weight = {}\n",
    "    \n",
    "    for word, weight in word_to_weight.items():\n",
    "\n",
    "        if not word: continue\n",
    "        try:\n",
    "            weight = float(weight)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        length = len(word)\n",
    "        if length not in length_word_weight:\n",
    "            length_word_weight[length] = {}\n",
    "        length_word_weight[length][word] = weight\n",
    "        \n",
    "    length_word_weight = sorted(length_word_weight.items())\n",
    "    \n",
    "    return length_word_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, {'台G': 1.0, '開噴': 2.0, '崩盤': 1.0, '空單': 1.0, '中美': 1.0, '支持': 1.0, '華為': 1.0, '川普': 1.0, '下探': 1.0, '上探': 1.0, '不要': 1.0}), (3, {'台GG': 2.0})]\n"
     ]
    }
   ],
   "source": [
    "word_to_weight = {\n",
    "    \"台G\": 1,\n",
    "    \"台GG\": 2,\n",
    "    \"開噴\": 2,\n",
    "    \"崩盤\": 1,\n",
    "    \"空單\": 1,\n",
    "    \"中美\": 1,\n",
    "    \"支持\": 1,\n",
    "    \"華為\": 1,\n",
    "    \"川普\": 1,\n",
    "    \"下探\": 1,\n",
    "    \"上探\": 1,\n",
    "    \"不要\": 1,\n",
    "}\n",
    "dictionary = construct_dictionary(word_to_weight)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "deliminate = ['COMMACATEGORY','COLONCATEGORY','DASHCATEGORY','ETCCATEGORY','EXCLAMATIONCATEGORY'\n",
    "              ,'PARENTHESISCATEGORY','PAUSECATEGORY','PERIODCATEGORY','QUESTIONCATEGORY','SEMICOLONCATEGOR'\n",
    "              ,'SPCHANGECATEGORY','WHITESPACE','DE','SHI','V_2','Caa','T','Neu','P','Caa','Cab','Cba','Cbb','Nes','Nep','Neqa','Neqb'\n",
    "             ,'Da','Dfa','Dfb']\n",
    "stopword = ['吧','就','都','也','者','/','跟','╱','又','了','才','得','台積','面板'\n",
    "            ,'半導體','台G','GG','台GG','蘋概股','晶圓','奈米','製程','貿易戰','中美'\n",
    "           ,'大盤','2330','台gg','gg','台績','IC產業','IC設計','科技股','那斯達克','納斯達克'\n",
    "            ,'道瓊','台積電','台績電','GG ADR','GG adr','gg ADR','gg adr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2.<b>排除遺失值&選擇資料</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#009933\"> >>優化點1：該用甚麼字篩出符合台積電的文章  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_small = all_small.dropna()\n",
    "GG = all_small[(all_small['內容'].str.contains('台積|面板|半導體|台G|GG|蘋概股|晶圓|奈米|製程|貿易戰|中美|大盤|2330|台gg|台g|gg|台績|IC產業|IC設計|科技股|納斯達克|那斯達克|道瓊')==True) & (all_small['標題'].str.contains('[公告]')==False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_small[(all_small['內容'].str.contains('川普&跪')==True) & (all_small['標題'].str.contains('[公告]')==False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>3.<b>確認迴圈時間</b></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date = list(set(GG['日期'].value_counts().index)) #抓出時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Date.sort()       #讓資料照時間排\n",
    "Date = Date[1:]   #確保資料為5/29-11/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>4.<b>實作tf</b></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#009933\">  >>優化點2：斷詞能否更準 (到最後他會存成很多csv，去預設資料夾看他切出的詞與分數，是還不錯但仍不好) </p>\n",
    "<p style=\"color:#009933\">  >>優化點3：tf-idf算法是否仍可改良？ 會不會導致重要的字反而分數較低？例如「開噴」，若每筆文章都提到，代表明天股價應該會下降，但idf會認為他在不同文章出現次數過高，反而使分數下降 </p>\n",
    "<p style=\"color:#009933\">  >>優化點4：我是以\"天\"為單位，把所有文章總和起來，給每天一個各詞的tf-idf值，所以和tf-idf原先的目的(找出能夠代表這篇文章的重要詞彙)並不相同 </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#double list\n",
    "com = [[a+'('+b+')' for a,b in zip(i,j)] for i,j in zip(ws_results, pos_results)]\n",
    "com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【2019-05-29】\n",
      ">> 共有 6文章 / 181詞 / 235詞數\n",
      ">> 次數Dataframe完成\n",
      ">> tf矩陣完成\n",
      ">> idf矩陣完成\n",
      ">> tf-idf矩陣完成\n",
      ">>>>>> 結束並存檔 (共5.03秒)\n",
      "【2019-05-30】\n",
      ">> 共有 246文章 / 3834詞 / 10942詞數\n",
      ">> 次數Dataframe完成\n",
      ">> tf矩陣完成\n",
      ">> idf矩陣完成\n",
      ">> tf-idf矩陣完成\n",
      ">>>>>> 結束並存檔 (共125.87秒)\n",
      "【2019-05-31】\n",
      ">> 共有 217文章 / 3267詞 / 9997詞數\n",
      ">> 次數Dataframe完成\n",
      ">> tf矩陣完成\n",
      ">> idf矩陣完成\n",
      ">> tf-idf矩陣完成\n",
      ">>>>>> 結束並存檔 (共219.09秒)\n"
     ]
    }
   ],
   "source": [
    "from ckiptagger import WS, POS, NER\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "for date in Date[0:3]:\n",
    "    tStart = time.time()#計時開始\n",
    "    print('【'+date+'】')\n",
    "    \n",
    "    #第一部分： 【斷詞+計算當天詞集+計算當天出現詞數】\n",
    "    ### 詞數 v.s 詞集:  詞數可以重複，詞集不行  (例如 1th、3th文章都有 \"台積\" ， 那詞數會是 2個 、詞集會是 1個\n",
    "    ws_results = ws(GG[GG['日期']==date]['內容'],sentence_segmentation=False,recommend_dictionary = dictionary)\n",
    "    pos_results = pos(ws_results)\n",
    "  #  result_com = [[a+'('+b+') 'for a,b in zip(k,v)]for k,v in zip(ws_results,pos_results)]\n",
    "    word_list = [] #出現詞數 (總和)\n",
    "    word_set = []  #當天詞集 (單一)\n",
    "    word_adj_only = []  #當天詞性 (單一)\n",
    "    word_adj_all = []   #當天詞性 (總和)\n",
    "    word_com_only = [] #單一總和\n",
    "    word_com_all = []  #全部次數\n",
    "    word_com_like = []  #合併ws,pos\n",
    "    today_num = len(ws_results) #當天文章數輛 (長度)\n",
    "    \n",
    "    for counter in range(today_num):\n",
    "        zip_all = zip(ws_results[counter],pos_results[counter])\n",
    "        word_temporary = []\n",
    "        for word,adj in zip_all:\n",
    "            if (adj not in deliminate and word not in stopword):\n",
    "                word_list.append(word.strip())\n",
    "                word_adj_all.append(adj.strip())\n",
    "                word_temporary.append(word.strip()+'('+adj.strip()+')')\n",
    "            else:\n",
    "                continue\n",
    "        word_com_like.append(word_temporary)\n",
    "        \n",
    "    for index in word_com_like:\n",
    "        for final_count in index:\n",
    "            word_com_all.append(final_count)\n",
    "  #  for article_nth in ws_results:\n",
    "  #      for word in article_nth :    \n",
    "  #          word_list.append(word)\n",
    "    word_set = list(set(word_list))\n",
    "    word_adj_only = list(set(word_adj_all))\n",
    "    word_com_only = list(set(word_com_all))\n",
    "    print('>> 共有 ' + str(today_num) + '文章'+ ' / ' + str(len(word_com_only)) + '詞' + ' / ' + str(len(word_com_all)) + '詞數')\n",
    "\n",
    "    #第二部分： 【創建空的Dataframe，欄位是當天有幾篇文章，列是當天的詞集數量】\n",
    "    zero_data = np.zeros(shape=(len(word_com_only),today_num))\n",
    "    globals()['columns'] = []               \n",
    "    for col in range(today_num):\n",
    "        columns.append(str(col+1)+'th')\n",
    "    total = pd.DataFrame(zero_data,index=word_com_only,columns=columns)\n",
    "    check_word = list(total.index)\n",
    "    \n",
    "    ##算次數\n",
    "    ###########################################################################################\n",
    "    ###########################################################################################\n",
    "    for total_col in range(today_num):\n",
    "        for text_pos in word_com_like[total_col]:\n",
    "            if text_pos  in check_word:\n",
    "                total.iloc[check_word.index(text_pos),total_col] +=1\n",
    "            else:\n",
    "                continue\n",
    "    print('>> 次數Dataframe完成')\n",
    "    ###########################################################################################\n",
    "    ###########################################################################################\n",
    "    \n",
    "    #深複製tf - idf (避免一般複製的標籤行為)\n",
    "    tf = copy.deepcopy(total)\n",
    "    idf = copy.deepcopy(total)\n",
    "    \n",
    "    #tf矩陣計算\n",
    "    for tf_col in range(len(tf.columns)):\n",
    "        col_sum = tf.sum(axis=0)[tf_col]\n",
    "        for tf_row in range(len(tf)):\n",
    "            if (tf.iloc[tf_row,tf_col] != 0):\n",
    "                tf.iloc[tf_row,tf_col] = round(tf.iloc[tf_row,tf_col]/col_sum,6)\n",
    "    print('>> tf矩陣完成')\n",
    "    \n",
    "    #idf矩陣成形\n",
    "    for idf_col in range(len(idf.columns)):\n",
    "        for idf_text_rol in idf.iloc[np.where(idf.iloc[:,idf_col]!=0)[0],:].index:\n",
    "            idf.iloc[np.where(idf.index==idf_text_rol)[0],idf_col]=(math.log(len(ws_results)/len(np.where(idf.loc[idf_text_rol]!=0)[0])))\n",
    "    print('>> idf矩陣完成')\n",
    "    \n",
    "    #tf idf互乘\n",
    "    tfidf = pd.DataFrame(np.array(tf)*np.array(idf),columns=columns,index=word_com_only)\n",
    "    print('>> tf-idf矩陣完成')\n",
    "    \n",
    "    #給出每天的 詞-tfidf分數 資料表\n",
    "    tfidf_text = pd.DataFrame(tfidf.mean(axis=1),columns=['分數']) #列平均\n",
    "    word = copy.deepcopy(tfidf_text.sort_values('分數',ascending=False).index)\n",
    "    score = copy.deepcopy(tfidf_text.sort_values('分數',ascending=False)['分數'])\n",
    "    pd.DataFrame({'詞':word,'分數':score}).to_csv(date+'.csv',encoding='utf_8_sig',index=False)\n",
    "    tEnd = time.time()#計時結束\n",
    "    print('>>>>>> 結束並存檔 (共' + str(round(tEnd-tStart,2)) + '秒)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
